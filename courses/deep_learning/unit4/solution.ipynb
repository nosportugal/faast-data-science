{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nosportugal/faast-data-science/blob/main/courses/deep_learning/unit4/solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFz8RPFScSDK"
   },
   "source": [
    "# Unit 4: Optimizing Neural Nets\n",
    "\n",
    "Your challenge in this unit will be to improve on the solution from the previous unit using a similar model architecture (fully connected layers). Trying different number of features or different optimizer and training hyperparameters is encouraged.\n",
    "\n",
    "By now, you should have the files `labeledTrainData.tsv` and `testData.tsv` in a folder named `ldsa-dl-course-data` in your Google Drive. If you don't, please check the README file of Unit 2 for instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y41LWa1cdAe"
   },
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6Mw06KGwJLF"
   },
   "outputs": [],
   "source": [
    "!pip install lightning==2.0.1 wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyskYQFqt6cn"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUiZ2U2Rpe9-"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# This will open a window so you can login to W&B on Google Colab.\n",
    "# If that doesn't work, set your W&B API key below\n",
    "# If you do, remove your key before publishing to GitHub.\n",
    "\n",
    "# %env WANDB_API_KEY=YOUR_WANDB_API_KEY\n",
    "wandb.login()\n",
    "run = wandb.init(project=\"imdb_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkQiAMu-ciVm"
   },
   "source": [
    "## 2) Load the train **dataset**\n",
    "\n",
    "Load the train dataset from the tsv files stored in your Google Drive. Split it into train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faEVXt-6wDLm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCMtlWpEualV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/labeledTrainData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")\n",
    "\n",
    "df_shuffled = df.sample(frac=1, random_state=1).reset_index()\n",
    "\n",
    "df_train = df_shuffled.iloc[:20000]\n",
    "df_val = df_shuffled.iloc[20000:25000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTfC_ptOcvMi"
   },
   "source": [
    "## 3) Vectorization\n",
    "\n",
    "Use Bag-of-Words for vectorizing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3LfMqlgcuVV"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CTgCYu0v4o1"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(lowercase=True, max_features=2000, stop_words=\"english\")\n",
    "\n",
    "cv.fit(df_train[\"review\"])\n",
    "\n",
    "X_train = cv.transform(df_train[\"review\"])\n",
    "X_val = cv.transform(df_val[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDiRJ1yOc5F6"
   },
   "source": [
    "## 4) Data loader\n",
    "\n",
    "Create a data PyTorch `Dataset` and corresponding `DataLoader` for the train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYu6tbn6c59n"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YH1HQxGyhvf"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPFzDPKEyip-"
   },
   "outputs": [],
   "source": [
    "train_ds = TextDataset(X_train.todense(), df_train[\"sentiment\"].values)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VK8xgno50sYT"
   },
   "outputs": [],
   "source": [
    "val_ds = TextDataset(X_val.todense(), df_val[\"sentiment\"].values)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxwL2TXb0vdb"
   },
   "outputs": [],
   "source": [
    "for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "    break\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hd1OQTBZdB93"
   },
   "source": [
    "## 5) Model definition\n",
    "\n",
    "Define a PyTorch model and the corresponding PyTorch Lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ts-ICWRDPn3D"
   },
   "outputs": [],
   "source": [
    "class PyTorchMLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer_size=10, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.all_layers = torch.nn.Sequential(\n",
    "            # Hidden layer, with ReLU activation.\n",
    "            torch.nn.Linear(num_features, hidden_layer_size),\n",
    "            torch.nn.ReLU(),\n",
    "            # Output layer.\n",
    "            torch.nn.Linear(hidden_layer_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        logits = self.all_layers(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "pytorch_model = PyTorchMLP(\n",
    "    num_features=2000,\n",
    "    hidden_layer_size=10,\n",
    "    num_classes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNdmz9rqjDBn"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from lightning import LightningModule\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn6g8M5n1YBi"
   },
   "outputs": [],
   "source": [
    "class LightningModel(LightningModule):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch\n",
    "        logits = self(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        return loss, true_labels, predicted_labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\n",
    "            \"train_acc\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPYI_mwFmkfN"
   },
   "source": [
    "## 6) Model training\n",
    "\n",
    "Train your model using a Lightning trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjVAQl1W1c7z"
   },
   "outputs": [],
   "source": [
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfnhI8ap1MHz"
   },
   "outputs": [],
   "source": [
    "lightning_model = LightningModel(model=pytorch_model, learning_rate=0.001)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(save_top_k=1, mode=\"max\", monitor=\"val_acc\", save_last=True)\n",
    "]\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"imdb_sentiment\",\n",
    "    log_model=\"all\",\n",
    "    group=\"unit4\",\n",
    ")\n",
    "\n",
    "wandb_logger.log_hyperparams(\n",
    "    {\n",
    "        \"optimizer\": lightning_model.configure_optimizers().__class__.__name__,\n",
    "        \"architecture\": str(lightning_model.model.all_layers),\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=30,\n",
    "    accelerator=\"auto\",\n",
    "    logger=wandb_logger,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=lightning_model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAG-FkdempyC"
   },
   "source": [
    "## 7) Inference\n",
    "\n",
    "Load the test dataset from the tsv file stored in your Google Drive and the model from the checkpoints you created on W&B. Finally, perform inference with the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D34007yatD2B"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/testData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")\n",
    "\n",
    "X_test = cv.transform(df_test[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJRXDD16xb3F"
   },
   "outputs": [],
   "source": [
    "class InferenceTextDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_7MSXUR08ar"
   },
   "outputs": [],
   "source": [
    "test_ds = InferenceTextDataset(X_test.todense())\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DmzG87btdcS"
   },
   "outputs": [],
   "source": [
    "# Define checkpoint reference.\n",
    "checkpoint_reference = \"[USERNAME]/imdb_sentiment/model-[MODEL_ID]:best\"\n",
    "\n",
    "# Download checkpoint locally (if not already cached).\n",
    "artifact = run.use_artifact(checkpoint_reference, type=\"model\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load checkpoint.\n",
    "model = LightningModel.load_from_checkpoint(str(artifact_dir) + \"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhQlLInExpTw"
   },
   "outputs": [],
   "source": [
    "batch_outputs = trainer.predict(model=model, dataloaders=test_loader)\n",
    "logits = torch.cat(batch_outputs)\n",
    "predicted_labels = torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KECKI2-rtfhe"
   },
   "source": [
    "## 8) Post-process for Kaggle submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the predicted class labels are stored in `predicted_labels` (as a Torch tensor), create a csv file ready for submission on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBaWfaqcbl2L"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\": df_test[\"id\"], \"sentiment\": predicted_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTD9pSBtTfIG"
   },
   "outputs": [],
   "source": [
    "output.to_csv(\"output.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
