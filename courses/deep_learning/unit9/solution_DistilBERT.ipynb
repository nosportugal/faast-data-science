{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nosportugal/faast-data-science/blob/main/courses/deep_learning/unit9/solution_DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFz8RPFScSDK"
   },
   "source": [
    "# Unit 9: Pre-trained Models\n",
    "\n",
    "By now, you should have the files `labeledTrainData.tsv` and `testData.tsv` in a folder named `ldsa-dl-course-data` in you Google Drive. If you don't, please check the README file of Unit 2 for instructions.\n",
    "\n",
    "Your chalenge in this unit will be to classify the sentiment of IMDb movie reviews using a fine tuned pre-trained model accessed from the Hugging Face platform.\n",
    "\n",
    "We recommend you to use W&B to track your experiments. Sign up with your google account so that connection with the Google Colab environment is seamless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y41LWa1cdAe"
   },
   "source": [
    "## 1) Setup & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6Mw06KGwJLF"
   },
   "outputs": [],
   "source": [
    "! pip install lightning==2.0.1 wandb datasets transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyskYQFqt6cn"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUiZ2U2Rpe9-"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# This will open a window so you can login to W&B on Google Colab.\n",
    "# If that doesn't work, set your W&B API key below\n",
    "# If you do, remove your key before publishing to GitHub.\n",
    "\n",
    "# %env WANDB_API_KEY=YOUR_WANDB_API_KEY\n",
    "wandb.login()\n",
    "run = wandb.init(project=\"imdb_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkQiAMu-ciVm"
   },
   "source": [
    "## 2) Load the **dataset**\n",
    "\n",
    "Load the train dataset from the tsv files stored in your Google Drive. Split it into train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faEVXt-6wDLm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCMtlWpEualV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/labeledTrainData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")\n",
    "\n",
    "df_shuffled = df.sample(frac=1, random_state=1).reset_index()\n",
    "\n",
    "df_train = df_shuffled.iloc[:20000]\n",
    "df_val = df_shuffled.iloc[20000:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/testData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1UMWV_Qt9BM"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "full_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_pandas(df_train),\n",
    "        \"validation\": Dataset.from_pandas(df_val),\n",
    "        \"test\": Dataset.from_pandas(df_test),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skcFTxUNvPIV"
   },
   "outputs": [],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTfC_ptOcvMi"
   },
   "source": [
    "## 3) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lxYeV84tj30"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased\", model_max_length=512\n",
    ")\n",
    "\n",
    "print(\"Tokenizer input max. length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9Nb96AztxMB"
   },
   "outputs": [],
   "source": [
    "full_dataset_tokenized = full_dataset.map(\n",
    "    lambda batch: tokenizer(batch[\"review\"], truncation=True, padding=True),\n",
    "    batched=True,\n",
    "    batch_size=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ln3c7jf3vom_"
   },
   "outputs": [],
   "source": [
    "full_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeMOPSUewenL"
   },
   "outputs": [],
   "source": [
    "for feature in [\"input_ids\", \"attention_mask\", \"sentiment\"]:\n",
    "    print(full_dataset_tokenized[\"train\"][0][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPdNlM4BxUaj"
   },
   "outputs": [],
   "source": [
    "full_dataset_tokenized[\"train\"].set_format(\n",
    "    \"torch\", columns=[\"input_ids\", \"attention_mask\", \"sentiment\"]\n",
    ")\n",
    "full_dataset_tokenized[\"validation\"].set_format(\n",
    "    \"torch\", columns=[\"input_ids\", \"attention_mask\", \"sentiment\"]\n",
    ")\n",
    "full_dataset_tokenized[\"test\"].set_format(\n",
    "    \"torch\", columns=[\"input_ids\", \"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtmDJa-EyBxS"
   },
   "outputs": [],
   "source": [
    "for feature in [\"input_ids\", \"attention_mask\", \"sentiment\"]:\n",
    "    print(full_dataset_tokenized[\"train\"][0][feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDiRJ1yOc5F6"
   },
   "source": [
    "## 4) Data loader\n",
    "\n",
    "Create a data PyTorch `Dataset` and corresponding `DataLoader` for the train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYu6tbn6c59n"
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YH1HQxGyhvf"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPFzDPKEyip-"
   },
   "outputs": [],
   "source": [
    "train_ds = TextDataset(full_dataset_tokenized[\"train\"])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VK8xgno50sYT"
   },
   "outputs": [],
   "source": [
    "val_ds = TextDataset(full_dataset_tokenized[\"validation\"])\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxwL2TXb0vdb"
   },
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTPJov_F0xKk"
   },
   "outputs": [],
   "source": [
    "print(batch)\n",
    "print(batch[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hd1OQTBZdB93"
   },
   "source": [
    "## 5) Model definition\n",
    "\n",
    "Define a PyTorch model and the corresponding PyTorch Lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGzNtYBMXQli"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "pytorch_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNdmz9rqjDBn"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn6g8M5n1YBi"
   },
   "outputs": [],
   "source": [
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        labels = batch[\"sentiment\"]\n",
    "\n",
    "        outputs = self(batch)\n",
    "        logits = outputs[\"logits\"]\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        return loss, labels, predicted_labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\n",
    "            \"train_acc\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Model training\n",
    "\n",
    "Train your model using a Lightning trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfnhI8ap1MHz"
   },
   "outputs": [],
   "source": [
    "lightning_model = LightningModel(model=pytorch_model, learning_rate=5e-5)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(save_top_k=1, mode=\"max\", monitor=\"val_acc\", save_last=True)\n",
    "]\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"imdb_sentiment\",\n",
    "    log_model=\"all\",\n",
    "    group=\"unit9\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=3,\n",
    "    accelerator=\"auto\",\n",
    "    logger=wandb_logger,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=lightning_model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Inference\n",
    "\n",
    "Load the test dataset from the tsv file stored in your Google Drive and the model from the checkpoints you created on W&B. Finally, perform inference with the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2MCiBWPVqoN"
   },
   "outputs": [],
   "source": [
    "test_ds = TextDataset(full_dataset_tokenized[\"test\"])\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint reference.\n",
    "checkpoint_reference = \"[USERNAME]/imdb_sentiment/model-[MODEL_ID]:best\"\n",
    "\n",
    "# Download checkpoint locally (if not already cached).\n",
    "artifact = run.use_artifact(checkpoint_reference, type=\"model\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load checkpoint.\n",
    "model = LightningModel.load_from_checkpoint(str(artifact_dir) + \"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMPI75n9WEw7"
   },
   "outputs": [],
   "source": [
    "batch_outputs = trainer.predict(model=model, dataloaders=test_loader)\n",
    "logits = torch.cat([batch_output[\"logits\"] for batch_output in batch_outputs])\n",
    "predicted_labels = torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBaWfaqcbl2L"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Post-process for Kaggle submission\n",
    "\n",
    "Assuming the predicted class labels are stored in `predicted_labels` (as a Torch tensor), create a csv file ready for submission on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_DudWvzJkgY"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\": df_test[\"id\"], \"sentiment\": predicted_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
