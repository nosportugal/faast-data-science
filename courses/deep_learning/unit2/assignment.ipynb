{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nosportugal/faast-data-science/blob/main/courses/deep_learning/unit2/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFz8RPFScSDK"
   },
   "source": [
    "# Unit 2: Neural Networks\n",
    "\n",
    "Your challenge in this unit is to classify the sentiment expressed in [IMDb](https://www.imdb.com/) movie reviews using a neural network.\n",
    "\n",
    "By now, you should have the files `labeledTrainData.tsv` and `testData.tsv` in a folder named `ldsa-dl-course-data` in your Google Drive. If you don't, please check the README file of Unit 2 for instructions.\n",
    "\n",
    "This notebook already contains implementations of bag-of-Words, the data loaders and the model definition as per the videos 8.2. Make sure to carefully review the code and then implement the training loop using vanilla Pytorch following this [reference notebook](https://github.com/Lightning-AI/dl-fundamentals/blob/main/unit04-multilayer-nets/4.3-mlp-pytorch/4.3-mlp-pytorch-part3-5-mnist/4.3-mlp-pytorch-part5-mnist.ipynb) that was shown in 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y41LWa1cdAe"
   },
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyskYQFqt6cn"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUiZ2U2Rpe9-"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# This will open a window so you can login to W&B on Google Colab.\n",
    "# If that doesn't work, set your W&B API key below\n",
    "# If you do, remove your key before publishing to GitHub.\n",
    "\n",
    "# %env WANDB_API_KEY=YOUR_WANDB_API_KEY\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkQiAMu-ciVm"
   },
   "source": [
    "## 2) Load the train **dataset**\n",
    "\n",
    "Load the train dataset from the tsv files stored in your Google Drive. Split it into train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faEVXt-6wDLm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCMtlWpEualV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/labeledTrainData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")\n",
    "\n",
    "df_shuffled = df.sample(frac=1, random_state=1).reset_index()\n",
    "\n",
    "df_train = df_shuffled.iloc[:20000]\n",
    "df_val = df_shuffled.iloc[20000:25000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bWhrf6U-Hmd"
   },
   "source": [
    "## 3) Vectorization\n",
    "\n",
    "Use Bag-of-Words for vectorizing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(lowercase=True, max_features=10_000, stop_words=\"english\")\n",
    "\n",
    "cv.fit(df_train[\"review\"])\n",
    "\n",
    "X_train = cv.transform(df_train[\"review\"])\n",
    "X_val = cv.transform(df_val[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpZnSDY0-K7t"
   },
   "source": [
    "## 4) Data loader\n",
    "\n",
    "Create a data PyTorch `Dataset` and corresponding `DataLoader` for the train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextDataset(X_train.todense(), df_train[\"sentiment\"].values)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = TextDataset(X_val.todense(), df_val[\"sentiment\"].values)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "    break\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNRVvN4pFVIc"
   },
   "source": [
    "## 5) Model definition\n",
    "\n",
    "Define a PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = LogisticRegression(num_features=10_000, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWlOh7SgFY_j"
   },
   "source": [
    "## 6) Model training\n",
    "\n",
    "Define your training loop using vanilla PyTorch."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
