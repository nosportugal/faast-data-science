{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nosportugal/faast-data-science/blob/main/courses/deep_learning/unit3/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFz8RPFScSDK"
   },
   "source": [
    "# Unit 3: Lightning and W&B\n",
    "\n",
    "Your challenge in this unit will be to classify the sentiment expressed in [IMDb](https://www.imdb.com/) movie reviews using a neural network and to submit your results to [this Kaggle competition](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview).\n",
    "\n",
    "By now, you should have the files `labeledTrainData.tsv` and `testData.tsv` in a folder named `ldsa-dl-course-data` in your Google Drive. If you don't, please check the README file of Unit 2 for instructions.\n",
    "\n",
    "[This Jupyter notebook](https://github.com/Lightning-AI/dl-fundamentals/blob/main/unit08-large-language-models/8.2-bag-of-words/8.2-part2-bag-of-words-classifier.ipynb) from Unit 8.2 of the Deep Learning Fundamentals course should help you adapting your solution from the previous week to use Lightning. When you're happy with your results, make an inference on the test set and make your first submission to the Kaggle competition.\n",
    "\n",
    "We recommend that you to use [Weights & Biases](https://wandb.ai/site) (W&B) to track your experiments. Sign up on W&B with your Google account so that connection with the Google Colab environment is seamless. Follow the [documentation](https://docs.wandb.ai/guides/integrations/lightning) to integrate W&B with PyTorch Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y41LWa1cdAe"
   },
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6Mw06KGwJLF"
   },
   "outputs": [],
   "source": [
    "!pip install lightning==2.0.1 wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyskYQFqt6cn"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUiZ2U2Rpe9-"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# This will open a window so you can login to W&B on Google Colab.\n",
    "# If that doesn't work, set your W&B API key below\n",
    "# If you do, remove your key before publishing to GitHub.\n",
    "\n",
    "# %env WANDB_API_KEY=YOUR_WANDB_API_KEY\n",
    "wandb.login()\n",
    "run = wandb.init(project=\"imdb_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkQiAMu-ciVm"
   },
   "source": [
    "## 2) Load the train **dataset**\n",
    "\n",
    "Load the train dataset from the tsv files stored in your Google Drive. Split it into train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faEVXt-6wDLm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCMtlWpEualV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/labeledTrainData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")\n",
    "\n",
    "df_shuffled = df.sample(frac=1, random_state=1).reset_index()\n",
    "\n",
    "df_train = df_shuffled.iloc[:20000]\n",
    "df_val = df_shuffled.iloc[20000:25000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bWhrf6U-Hmd"
   },
   "source": [
    "## 3) Vectorization\n",
    "\n",
    "Use Bag-of-Words for vectorizing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(lowercase=True, max_features=10_000, stop_words=\"english\")\n",
    "\n",
    "cv.fit(df_train[\"review\"])\n",
    "\n",
    "X_train = cv.transform(df_train[\"review\"])\n",
    "X_val = cv.transform(df_val[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpZnSDY0-K7t"
   },
   "source": [
    "## 4) Data loader\n",
    "\n",
    "Create a data PyTorch `Dataset` and corresponding `DataLoader` for the train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextDataset(X_train.todense(), df_train[\"sentiment\"].values)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = TextDataset(X_val.todense(), df_val[\"sentiment\"].values)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "    break\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNRVvN4pFVIc"
   },
   "source": [
    "## 5) Model definition\n",
    "\n",
    "Define a PyTorch model and the corresponding PyTorch Lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(LightningModule):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWlOh7SgFY_j"
   },
   "source": [
    "## 6) Model training\n",
    "\n",
    "Train your model using a Lightning trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfWDXIx6Ffzf"
   },
   "source": [
    "## 7) Inference\n",
    "\n",
    "Load the test dataset from the tsv file stored in your Google Drive and the model from the checkpoints you created on W&B. Finally, perform inference with the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oMpvY-Cu0fc"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/testData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint reference.\n",
    "checkpoint_reference = \"[USERNAME]/imdb_sentiment/model-[MODEL_ID]:best\"\n",
    "\n",
    "# Download checkpoint locally (if not already cached).\n",
    "artifact = run.use_artifact(checkpoint_reference, type=\"model\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load checkpoint.\n",
    "model = LightningModel.load_from_checkpoint(str(artifact_dir) + \"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5F_KYmlFkN0"
   },
   "source": [
    "## 8) Post-process for Kaggle submission\n",
    "\n",
    "Assuming the predicted class labels are stored in `predicted_labels` (as a Torch tensor), create a csv file ready for submission on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dg1rQIPiFmVm"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\": df_test[\"id\"], \"sentiment\": predicted_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ur5OBYQiForw"
   },
   "outputs": [],
   "source": [
    "output.to_csv(\"output.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
