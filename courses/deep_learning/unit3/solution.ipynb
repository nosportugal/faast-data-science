{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TSFelg/dl-cohort/blob/main/unit3/solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFz8RPFScSDK"
   },
   "source": [
    "# Unit 3: Lightning and W&B\n",
    "\n",
    "Your challenge in this unit will be to classify the sentiment expressed in [IMDb](https://www.imdb.com/) movie reviews using a neural network and to submit your results to [this Kaggle competition](https://www.kaggle.com/competitions/word2vec-nlp-tutorial/overview).\n",
    "\n",
    "By now, you should have the files `labeledTrainData.tsv` and `testData.tsv` in a folder named `ldsa-dl-course-data` in your Google Drive. If you don't, please check the README file of Unit 2 for instructions.\n",
    "\n",
    "[This Jupyter notebook](https://github.com/Lightning-AI/dl-fundamentals/blob/main/unit08-large-language-models/8.2-bag-of-words/8.2-part2-bag-of-words-classifier.ipynb) from Unit 8.2 of the Deep Learning Fundamentals course should help you setting up a first baseline. Use Bag-of-Words for vectorizing your dataset and start with a 1-layer neural network with a linear activation function. When you're happy with your results, make an inference on the test set and make your first submission to the Kaggle competition.\n",
    "\n",
    "We recommend that you to use [Weights & Biases](https://wandb.ai/site) (W&B) to track your experiments. Sign up on W&B with your Google account so that connection with the Google Colab environment is seamless. Follow the [documentation](https://docs.wandb.ai/guides/integrations/lightning) to integrate W&B with PyTorch Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y41LWa1cdAe"
   },
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6Mw06KGwJLF"
   },
   "outputs": [],
   "source": [
    "!pip install lightning==2.0.1 wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyskYQFqt6cn"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUiZ2U2Rpe9-"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# This will open a window so you can login to W&B on Google Colab.\n",
    "# If that doesn't work, set your W&B API key below\n",
    "# If you do, remove your key before publishing to GitHub.\n",
    "\n",
    "# %env WANDB_API_KEY=YOUR_WANDB_API_KEY\n",
    "wandb.login()\n",
    "run = wandb.init(project=\"imdb_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkQiAMu-ciVm"
   },
   "source": [
    "## 2) Load the train **dataset**\n",
    "\n",
    "Load the train dataset from the tsv files stored in your Google Drive. Split it into train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faEVXt-6wDLm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCMtlWpEualV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/labeledTrainData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")\n",
    "\n",
    "df_shuffled = df.sample(frac=1, random_state=1).reset_index()\n",
    "\n",
    "df_train = df_shuffled.iloc[:20000]\n",
    "df_val = df_shuffled.iloc[20000:25000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTfC_ptOcvMi"
   },
   "source": [
    "## 3) Vectorization\n",
    "\n",
    "Use Bag-of-Words for vectorizing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3LfMqlgcuVV"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CTgCYu0v4o1"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(lowercase=True, max_features=10_000, stop_words=\"english\")\n",
    "\n",
    "cv.fit(df_train[\"review\"])\n",
    "\n",
    "X_train = cv.transform(df_train[\"review\"])\n",
    "X_val = cv.transform(df_val[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDiRJ1yOc5F6"
   },
   "source": [
    "## 4) Data loader\n",
    "\n",
    "Create a data PyTorch `Dataset` and corresponding `DataLoader` for the train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYu6tbn6c59n"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YH1HQxGyhvf"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPFzDPKEyip-"
   },
   "outputs": [],
   "source": [
    "train_ds = TextDataset(X_train.todense(), df_train[\"sentiment\"].values)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VK8xgno50sYT"
   },
   "outputs": [],
   "source": [
    "val_ds = TextDataset(X_val.todense(), df_val[\"sentiment\"].values)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxwL2TXb0vdb"
   },
   "outputs": [],
   "source": [
    "for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "    break\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hd1OQTBZdB93"
   },
   "source": [
    "## 5) Model definition\n",
    "\n",
    "Define a PyTorch model and the corresponding PyTorch Lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGzNtYBMXQli"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "pytorch_model = LogisticRegression(num_features=10_000, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNdmz9rqjDBn"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from lightning import LightningModule\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn6g8M5n1YBi"
   },
   "outputs": [],
   "source": [
    "class LightningModel(LightningModule):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch\n",
    "        logits = self(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        return loss, true_labels, predicted_labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\n",
    "            \"train_acc\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPYI_mwFmkfN"
   },
   "source": [
    "## 6) Model training\n",
    "\n",
    "Train your model using a Lightning trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjVAQl1W1c7z"
   },
   "outputs": [],
   "source": [
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfnhI8ap1MHz"
   },
   "outputs": [],
   "source": [
    "lightning_model = LightningModel(model=pytorch_model, learning_rate=0.05)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(save_top_k=1, mode=\"max\", monitor=\"val_acc\", save_last=True)\n",
    "]\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"imdb_sentiment\",\n",
    "    log_model=\"all\",\n",
    "    group=\"unit3\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=30,\n",
    "    accelerator=\"auto\",\n",
    "    logger=wandb_logger,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=lightning_model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAG-FkdempyC"
   },
   "source": [
    "## 7) Inference\n",
    "\n",
    "Load the test dataset from the tsv file stored in your Google Drive and the model from the checkpoints you created on W&B. Finally, perform inference with the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCnmWNND06sy"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/testData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")\n",
    "\n",
    "X_test = cv.transform(df_test[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJRXDD16xb3F"
   },
   "outputs": [],
   "source": [
    "class InferenceTextDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_7MSXUR08ar"
   },
   "outputs": [],
   "source": [
    "test_ds = InferenceTextDataset(X_test.todense())\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF0AXIUhERCN"
   },
   "outputs": [],
   "source": [
    "# Define checkpoint reference.\n",
    "checkpoint_reference = \"[USERNAME]/imdb_sentiment/model-[MODEL_ID]:best\"\n",
    "\n",
    "# Download checkpoint locally (if not already cached).\n",
    "artifact = run.use_artifact(checkpoint_reference, type=\"model\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load checkpoint.\n",
    "model = LightningModel.load_from_checkpoint(str(artifact_dir) + \"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhQlLInExpTw"
   },
   "outputs": [],
   "source": [
    "batch_outputs = trainer.predict(model=model, dataloaders=test_loader)\n",
    "logits = torch.cat(batch_outputs)\n",
    "predicted_labels = torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KECKI2-rtfhe"
   },
   "source": [
    "## 8) Post-process for Kaggle submission\n",
    "\n",
    "Assuming the predicted class labels are stored in `predicted_labels` (as a Torch tensor), create a csv file ready for submission on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBaWfaqcbl2L"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\": df_test[\"id\"], \"sentiment\": predicted_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ntTxLhsbsaw"
   },
   "outputs": [],
   "source": [
    "output.to_csv(\"output.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
