{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nosportugal/faast-data-science/blob/main/courses/deep_learning/unit5/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFz8RPFScSDK"
   },
   "source": [
    "# Unit 5: Word Embeddings\n",
    "\n",
    "By now, you should have the files `labeledTrainData.tsv` and `testData.tsv` in a folder named `ldsa-dl-course-data` in you Google Drive. If you don't, please check the README file of Unit 2 for instructions.\n",
    "\n",
    "We recommend that you to use [Weights & Biases](https://wandb.ai/site) (W&B) to track your experiments. Sign up on W&B with your Google account so that connection with the Google Colab environment is seamless. Follow the [documentation](https://docs.wandb.ai/guides/integrations/lightning) to integrate W&B with PyTorch Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y41LWa1cdAe"
   },
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6Mw06KGwJLF"
   },
   "outputs": [],
   "source": [
    "!pip install lightning==2.0.1 wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyskYQFqt6cn"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUiZ2U2Rpe9-"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# This will open a window so you can login to W&B on Google Colab.\n",
    "# If that doesn't work, set your W&B API key below\n",
    "# If you do, remove your key before publishing to GitHub.\n",
    "\n",
    "# %env WANDB_API_KEY=YOUR_WANDB_API_KEY\n",
    "wandb.login()\n",
    "run = wandb.init(project=\"imdb_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkQiAMu-ciVm"
   },
   "source": [
    "## 2) Load the train **dataset**\n",
    "\n",
    "Load the train dataset from the tsv files stored in your Google Drive. Split it into train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faEVXt-6wDLm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCMtlWpEualV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/labeledTrainData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")\n",
    "\n",
    "df_shuffled = df.sample(frac=1, random_state=1).reset_index()\n",
    "\n",
    "df_train = df_shuffled.iloc[:20000]\n",
    "df_val = df_shuffled.iloc[20000:25000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bWhrf6U-Hmd"
   },
   "source": [
    "## 3) Tokenization\n",
    "\n",
    "The goal of this section is to transform the data, such that each word is segmented and mapped to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Keras text Tokenizer, as it is quite simple to use for this end.\n",
    "# Note that this is a pre-processing step, which we can decouple from the model.\n",
    "# Keras is being used in a way similar to how sklearn was used in previous\n",
    "# units, simply as a means of doing data processing. The model side still uses\n",
    "# PyTorch only. Other alternatives for the processing are spaCy and torchtext.\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True\n",
    ")\n",
    "\n",
    "# The tokenizer looks at all the words that exist in the training dataset, and\n",
    "# assigns an integer to each unique word (up to the 10000 most common)\n",
    "tokenizer.fit_on_texts(df_train[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method first transforms each sentence into an array of integers. We also\n",
    "# add padding (extra zeros) so that the length of the sequences are consistent.\n",
    "def tokenize_to_array(texts, max_seq_len):\n",
    "    tokenized_texts = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    X = np.empty((len(texts), max_seq_len))\n",
    "    X[...] = 0\n",
    "\n",
    "    for i, tokenized_text in enumerate(tokenized_texts):\n",
    "        X[i, : len(tokenized_text)] = tokenized_text\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover the length of the longest sentence in the training dataset.\n",
    "train_texts = df[\"review\"]\n",
    "print(\n",
    "    f\"Max. sequence length on train dataset: {len(max(tokenizer.texts_to_sequences(train_texts), key=len))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 2200  # Add an extra margin.\n",
    "\n",
    "X_train = tokenize_to_array(df_train[\"review\"], max_seq_len)\n",
    "X_val = tokenize_to_array(df_val[\"review\"], max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a tokenized sentence.\n",
    "print(X_train.shape)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpZnSDY0-K7t"
   },
   "source": [
    "## 4) Data loader\n",
    "\n",
    "Create a data PyTorch `Dataset` and corresponding `DataLoader` for the train and validation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNRVvN4pFVIc"
   },
   "source": [
    "## 5) Model definition\n",
    "\n",
    "Define a PyTorch model and the corresponding PyTorch Lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(LightningModule):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWlOh7SgFY_j"
   },
   "source": [
    "## 6) Model training\n",
    "\n",
    "Train your model using a Lightning trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfWDXIx6Ffzf"
   },
   "source": [
    "## 7) Inference\n",
    "\n",
    "Load the test dataset from the tsv file stored in your Google Drive and the model from the checkpoints you created on W&B. Finally, perform inference with the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oMpvY-Cu0fc"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    \"/content/drive/My Drive/ldsa-dl-course-data/testData.tsv\",\n",
    "    header=0,\n",
    "    delimiter=\"\\t\",\n",
    "    quoting=3,\n",
    ")\n",
    "\n",
    "X_test = tokenize_to_array(df_test[\"review\"], max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint reference.\n",
    "checkpoint_reference = \"[USERNAME]/imdb_sentiment/model-[MODEL_ID]:best\"\n",
    "\n",
    "# Download checkpoint locally (if not already cached).\n",
    "artifact = run.use_artifact(checkpoint_reference, type=\"model\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load checkpoint.\n",
    "model = LightningModel.load_from_checkpoint(str(artifact_dir) + \"/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5F_KYmlFkN0"
   },
   "source": [
    "## 8) Post-process for Kaggle submission\n",
    "\n",
    "Assuming the predicted class labels are stored in `predicted_labels` (as a Torch tensor), create a csv file ready for submission on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dg1rQIPiFmVm"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\": df_test[\"id\"], \"sentiment\": predicted_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ur5OBYQiForw"
   },
   "outputs": [],
   "source": [
    "output.to_csv(\"output.csv\", index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
